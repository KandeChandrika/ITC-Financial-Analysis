# -*- coding: utf-8 -*-
"""ITC-Finance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VEWoYNcyutyEJ5FK5WypMAU2a2ZBdELn
"""

! pip install chromadb

!pip install pymupdf tavily-python

from langchain.docstore.document import Document

TravilyKey= "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"

from tavily import TavilyClient
from langchain.schema import Document

client = TavilyClient(TravilyKey)

# Define your PDF sources and corresponding metadata
pdf_sources = [
    {
        "url": "https://www.itcportal.com/about-itc/shareholder-value/annual-reports/itc-annual-report-2023/pdf/ITC-Report-and-Accounts-2023.pdf",
        "metadata": {"source": "ITC Report and Accounts 2023"}
    },
    {
        "url": "https://www.itcportal.com/about-itc/shareholder-value/annual-reports/itc-annual-report-2024/pdf/ITC-Report-and-Accounts-2024.pdf",
        "metadata": {"source": "ITC Report and Accounts 2024"}
    },
    {
        "url": "https://www.itcportal.com/investor/pdf/ITC-Quarterly-Result-Presentation-Q1-FY2024.pdf",
        "metadata": {"source": "ITC Quarterly Report â€“ Q1 FY2024"}
    },
    {
        "url": "https://www.itcportal.com/investor/pdf/ITC-Quarterly-Result-Presentation-Q2-FY2024.pdf",
        "metadata": {"source": "ITC Quarterly Report â€“ Q2 FY2024"}
    },
    {
        "url": "https://www.itcportal.com/investor/pdf/ITC-Quarterly-Result-Presentation-Q3-FY2024.pdf",
        "metadata": {"source": "ITC Quarterly Report â€“ Q3 FY2024"}
    },
    {
        "url": "https://www.itcportal.com/investor/pdf/ITC-Quarterly-Result-Presentation-Q4-FY2024.pdf",
        "metadata": {"source": "ITC Quarterly Report â€“ Q4 FY2024"}
    },
    {
        "url": "https://www.itcportal.com/investor/pdf/ITC-Quarterly-Result-Presentation-Q1-FY2025.pdf",
        "metadata": {"source": "ITC Quarterly Report â€“ Q1 FY2025"}
    },
    {
        "url": "https://www.itcportal.com/investor/pdf/ITC-Quarterly-Result-Presentation-Q2-FY2025.pdf",
        "metadata": {"source": "ITC Quarterly Report â€“ Q2 FY202"}
    },
    {
        "url": "https://www.itcportal.com/investor/pdf/ITC-Quarterly-Result-Presentation-Q3-FY2025.pdf",
        "metadata": {"source": "ITC Quarterly Report â€“ Q3 FY2025"}
    },

]

# Loop through all PDFs, extract content, and create Document objects
docs = []
for pdf in pdf_sources:
    outcome = client.extract(urls=[pdf["url"]], extract_depth="advanced")
    raw_content = outcome["results"][0]["raw_content"]
    docs.append(Document(page_content=raw_content, metadata=pdf["metadata"]))

# Optional: Print one sample
print(docs[0])

len(docs)

import re

def cleaning(txt):

    # Lower
    txt = txt.lower()

    # Remove HTML tags
    txt = re.sub(r'<[^>]+>', ' ', txt)

    # Remove URLs (http, https, www)
    txt = re.sub(r'http\S+|www\.\S+', ' ', txt)

    # Remove Markdown links: [text](link)
    txt = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', txt)

    # Remove HTML links: <a href="...">text</a> (already removed by HTML tag regex, but just in case)
    txt = re.sub(r'<a\s+(?:[^>]?\s+)?href="[^"]">(.*?)</a>', r'\1', txt)

    return txt

# Applying Cleaning function on document text
for doc in docs:
  doc.page_content = cleaning(doc.page_content)

docs[0].page_content

# saving the docs
import pickle
from langchain.schema import Document

docs_to_save = docs

with open("final_docs.pkl", "wb") as f:
    pickle.dump(docs_to_save, f)





# from tavily import TavilyClient

# # Replace with your actual Tavily API key
# # client = TavilyClient(api_key="your_api_key_here") # This line had the placeholder API key.
# client = TavilyClient(api_key="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") # Replaced with the actual API key from previous cells

# # Step 1: Search the ITC reports page
# search_results = client.search(
#     query="ITC annual report 2023 site:itcportal.com",
#     include_raw_content=True
# )

# # Print raw content of the top result
# for result in search_results.get("results", []):
#     print("\nðŸ”— URL:", result["url"])
#     print("ðŸ“„ Raw Content (truncated):", result["content"][:1000])  # show first 1000 chars

# # To install: pip install tavily-python
# from tavily import TavilyClient
# client = TavilyClient("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")
# response = client.extract(
#     urls=["https://www.itcportal.com/about-itc/shareholder-value/annual-reports/itc-annual-report-2024/pdf/ITC-Report-and-Accounts-2024.pdf","https://www.itcportal.com/about-itc/shareholder-value/annual-reports/itc-annual-report-2023/pdf/ITC-Report-and-Accounts-2023.pdf"]
# )
# print(response)











# import os  # For accessing environment variables
# import re  # For finding PDF links with regular expressions
# import requests  # For downloading PDFs
# import pdfplumber  # For extracting text from PDFs
# from tavily import TavilyClient  # For searching with Tavily AI
# from pathlib import Path  # For handling file paths
# from dotenv import load_dotenv  # For loading API keys from .env file
# from urllib.parse import urlparse, parse_qs  # For URL normalization

# # Load environment variables from .env file
# load_dotenv()
# # Get the Tavily API key from the .env file
# TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

# # Initialize the Tavily client with the API key
# client = TavilyClient(api_key=TAVILY_API_KEY)

# # Create a directory to store downloaded PDFs and text files
# SCRAPED_DATA_DIR = Path("scraped_data")
# SCRAPED_DATA_DIR.mkdir(exist_ok=True)  # Create if it doesn't exist

# def normalize_url(url):
#     """
#     Normalize a URL by converting to lowercase and removing query parameters.
#     Returns the normalized URL.
#     """
#     parsed = urlparse(url)
#     # Keep only scheme, netloc, and path
#     clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
#     return clean_url.lower()

# def scrape_itc_reports():
#     """
#     Search for ITC's annual reports (2023, 2024) using Tavily AI.
#     Returns a list of unique PDF URLs.
#     """
#     try:
#         # Search for reports on itcportal.com
#         search_results = client.search(
#             query="ITC annual report 2023 2024 site:itcportal.com",  # Look for 2023 and 2024 reports
#             include_raw_content=True,  # Get full page content
#             max_results=10  # Limit to 10 results for efficiency
#         )

#         report_links = []
#         # Loop through each search result
#         for result in search_results.get("results", []):
#             content = result.get("content", "")  # Get the page content
#             url = result.get("url", "")  # Get the page URL

#             # Find all PDF links in the content using regex
#             pdf_urls = re.findall(r'(https?://[^\s]+\.pdf)', content)
#             # Filter for 2023 or 2024 reports and normalize URLs
#             for pdf_url in pdf_urls:
#                 if any(year in pdf_url.lower() for year in ["2023", "2024"]):
#                     report_links.append(normalize_url(pdf_url))

#             # If the result URL itself is a PDF for 2023 or 2024, include it
#             if url.endswith(".pdf") and any(year in url.lower() for year in ["2023", "2024"]):
#                 report_links.append(normalize_url(url))

#         # Remove duplicate links
#         unique_links = list(set(report_links))
#         print("Found PDF links:", unique_links)  # Log links for debugging
#         return unique_links

#     except Exception as e:
#         # Print error if search fails
#         print(f"Error during Tavily search: {e}")
#         return []

# def download_pdf(url, filename):
#     """
#     Download a PDF from the given URL and save it to scraped_data/.
#     Returns the file path or None if download fails.
#     """
#     try:
#         # Send HTTP request to get the PDF
#         response = requests.get(url, stream=True)
#         response.raise_for_status()  # Raise error if request fails
#         # Define the file path
#         file_path = SCRAPED_DATA_DIR / filename
#         # Save the PDF content to the file
#         with open(file_path, "wb") as f:
#             for chunk in response.iter_content(chunk_size=8192):
#                 f.write(chunk)
#         print(f"Downloaded: {file_path}")
#         return file_path
#     except Exception as e:
#         # Print error if download fails
#         print(f"Error downloading {url}: {e}")
#         return None

# def extract_text_from_pdf(pdf_path):
#     """
#     Extract raw text from a PDF file.
#     Returns the text as a string.
#     """
#     try:
#         # Open the PDF with pdfplumber
#         with pdfplumber.open(pdf_path) as pdf:
#             text = ""
#             # Loop through each page and extract text
#             for page in pdf.pages:
#                 page_text = page.extract_text()
#                 if page_text:  # Only add if text exists
#                     text += page_text + "\n"
#         return text
#     except Exception as e:
#         # Print error if text extraction fails
#         print(f"Error extracting text from {pdf_path}: {e}")
#         return ""

# def save_text_to_file(text, filename):
#     """
#     Save extracted text to a .txt file in scraped_data/.
#     Returns the file path or None if saving fails.
#     """
#     text_path = SCRAPED_DATA_DIR / filename
#     try:
#         # Save the text to a file with UTF-8 encoding
#         with open(text_path, "w", encoding="utf-8") as f:
#             f.write(text)
#         print(f"Saved text: {text_path}")
#         return text_path
#     except Exception as e:
#         # Print error if saving fails
#         print(f"Error saving text to {text_path}: {e}")
#         return None

# def main():
#     """
#     Main function to scrape, download, and extract text from ITC annual reports.
#     """
#     # Get PDF links for 2023 and 2024 reports
#     report_links = scrape_itc_reports()

#     # Check if any links were found
#     if not report_links:
#         print("No report links found.")
#         return

#     # Track processed years to avoid duplicates
#     processed_years = set()

#     # Process each PDF link
#     for link in report_links:
#         # Determine the year from the URL
#         year = "2023" if "2023" in link.lower() else "2024" if "2024" in link.lower() else "unknown"

#         # Skip if this year has already been processed
#         if year in processed_years:
#             print(f"Skipping duplicate report for {year}")
#             continue

#         # Define filenames for PDF and text
#         pdf_filename = f"itc_annual_report_{year}.pdf"
#         text_filename = f"itc_annual_report_{year}.txt"

#         # Download the PDF
#         pdf_path = download_pdf(link, pdf_filename)
#         if pdf_path:
#             # Extract text from the PDF
#             text = extract_text_from_pdf(pdf_path)
#             if text:
#                 # Save the text to a file
#                 save_text_to_file(text, text_filename)
#                 # Mark this year as processed
#                 processed_years.add(year)

# if __name__ == "__main__":
#     # Run the main function when the script is executed
#     # Write API key to .env file (only needed for initial setup)
#     with open('.env', 'w') as f:
#         f.write("TAVILY_API_KEY=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")
#     main()

