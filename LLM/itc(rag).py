# -*- coding: utf-8 -*-
"""ITC(RAG).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zIXbgDadpDJXQmJPClyGH_aPSYHDcWs4

Financial Analysis Using RAG Application on ITC

- Document Loaders
- Text Splitterss
- Vector Stores
- Retrievers
"""



! pip install -q youtube-transcript-api langchain langchain-core langchain-community langchain-google-genai faiss-cpu tiktoken python-dotenv

!pip install chromadb

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS, Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

apikey = "AIzaSyAFYSSFF6jUL2TJAqvWCduYLopNy-N3pP8"

from google.colab import userdata
userdata.get('api_key')

# 2# Unzip the folder after uploading
import zipfile

with zipfile.ZipFile('/content/chroma_db_backup.zip', 'r') as zip_ref:
    zip_ref.extractall('chroma_db')

vector_store = Chroma(
    persist_directory='chroma_db',
    embedding_function=embeddings
)

Retriever = vector_store.as_retriever(
    search_type = "mmr",
    search_kwargs = {"k":3, "lambda_mult":1} #{num_results, relevance-diversity-balance}
)

llm = ChatGoogleGenerativeAI(
    api_key = apikey,
    model="gemini-2.0-flash-exp")

from langchain.chains import RetrievalQA


QA = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=Retriever,
    return_source_documents=True
)

from langchain.chains import RetrievalQA


QA = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=Retriever,
    return_source_documents=True
)

query = "Summarize ITC's sustainability efforts inÂ 2024"
response = QA({"query": query})

print("Answer:")
print(response["result"])
print("\nSources:")
for doc in response["source_documents"]:
    print(doc.metadata)

