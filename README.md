# üìä ITC Sustainability Analyzer APP ‚Äì AI-Powered Financial Q&A

An interactive AI app to explore ITC Ltd‚Äôs **sustainability efforts in 2024**, combining vector embeddings, Google Gemini, and LangChain inside a simple and clean **Streamlit** interface. This app provides factual, citation-backed answers with traceability to original report documents.

---

## üåü Key Capabilities

- ü§ñ **Smart Embedding Loader**: Loads pre-processed financial documents from a local `Chroma` vector store.
- üß¨ **Embedding Engine**: Uses `GoogleGenerativeAIEmbeddings` for encoding financial text into vector format.
- üß† **Conversational AI**: Powered by `Gemini 2.0 Flash`, answering questions with precision and full citation.
- üñ•Ô∏è **Streamlit Interface**: A user-friendly frontend to interact with ITC‚Äôs sustainability insights.

---

## üîß Project Modules and Pipeline

---

### üìÅ 1. Data Scraping (Handled Separately)

> ‚ö†Ô∏è Not handled in the current app. The documents must be scraped manually or via a separate script using **Tavily**, **Firecrawl**, or **Crew AI**.

Example (Tavily):

```python
from tavily import Tavily

api_key = "your_tavily_api_key"
client = Tavily(api_key)

url = "https://www.itcportal.com/investor-relations/financial-reports/"
response = client.scrape_pdf(url)

extracted_text = response['text']
metadata = response['metadata']
print(extracted_text[:500])
print(metadata)

### üìÅ 2. Data Storage in Chroma Vector DB

The **scraped financial documents** (e.g., ITC‚Äôs reports) are **chunked**, **embedded**, and **stored locally** in a Chroma vector database, enabling efficient semantic search.

---

#### üìÇ Required Files in `./chroma_db`

The following files must be present in the `chroma_db` directory:

- `chroma.sqlite3` ‚Äì The main Chroma database file
- `index` files ‚Äì Metadata and vector index files for fast retrieval
- `data_level0.bin` ‚Äì Binary file containing document-level vector data
- Other auxiliary Chroma files (e.g., `lock`, `manifest.json`, etc.)

---

#### üîí Deployment Note

> These files must be **pre-generated** during embedding and should be **included with your deployment** (e.g., zipped as `chroma_db.zip`).

Once deployed, the app will load this Chroma vector store to enable question answering over the stored financial content.

### üß† 3. Embedding Layer (`/embeddings`)

This module transforms raw financial text into dense vector representations for semantic search and retrieval.

---

#### üîÑ Text Chunking

- Performed using `RecursiveCharacterTextSplitter` from **LangChain**
- Breaks long documents into overlapping, manageable chunks for better context retention

---

#### ‚öôÔ∏è Embedding Generation

- Uses **GoogleGenerativeAIEmbeddings** with the model `embedding-001`
- Requires a valid `GOOGLE_API_KEY` for authentication
- Produces dense vectors optimized for use with Google Gemini LLMs

---

#### üóÉÔ∏è Vector Storage

- Embedded chunks are stored in a **local Chroma vector database** located at:

---

#### üß≥ Deployment Optimization

- The `load_vector_store()` function in `streamlit_app.py` uses:
```python
@st.cache_resource

#### üß† 3. Embedding Layer (`/embeddings`)

This module transforms raw financial text into dense vector representations for semantic search and retrieval.

#### üîÑ Text Chunking

- Performed using `RecursiveCharacterTextSplitter` from **LangChain**
- Breaks long documents into overlapping, manageable chunks for better context retention

#### ‚öôÔ∏è Embedding Generation

- Uses **GoogleGenerativeAIEmbeddings** with the model `embedding-001`
- Requires a valid `GOOGLE_API_KEY` for authentication
- Produces dense vectors optimized for use with Google Gemini LLMs

#### üóÉÔ∏è Vector Storage

- Embedded chunks are stored in a **local Chroma vector database** located at:
  ```
  ./chroma_db
  ```

#### üß≥ Deployment Optimization

- The `load_vector_store()` function in `streamlit_app.py` uses:
  ```python
  @st.cache_resource
  ```


  ```
  to cache and reuse the vector store across sessions, reducing load time and boosting performance.

---

### üí¨ 4. LLM Query Interface (`/llm`)

- ü§ñ LLM: `ChatGoogleGenerativeAI` using **Gemini 2.0 Flash**
- üéØ Retriever: **MMR (Maximal Marginal Relevance)** for better diversity
- üìÑ Cites source documents, showing metadata and preview snippets
- üîí API key pulled securely via `st.secrets["GOOGLE_API_KEY"]`

---

### üñºÔ∏è 5. Streamlit Chat App (`streamlit_app.py`)

#### üéØ Goal: Visual frontend for real-time Q&A

##### Key Features:
- üí¨ Text input box for questions like:
  - ‚ÄúSummarize ITC's sustainability efforts in 2024.‚Äù
  - ‚ÄúList green initiatives by ITC in 2023‚Äì2024.‚Äù
- üîé AI-powered answer from Gemini
- üßæ Source documents shown with:
  - Metadata (year, type, etc.)
  - Preview of raw content

##### Sidebar:
- üìò About the app and tech stack
- üìã Instructions for usage
- üõ†Ô∏è Troubleshooting tips

---

## ‚úÖ Deployment Checklist

- [ ] Upload your `chroma_db` directory
- [ ] Add your `GOOGLE_API_KEY` to `.streamlit/secrets.toml`
- [ ] Run: `streamlit run streamlit_app.py`

---

## üß† Tech Stack

- **Streamlit** ‚Äì For the web interface
- **LangChain** ‚Äì For RAG and retrievers
- **Google Gemini 2.0 Flash** ‚Äì As the language model
- **Chroma DB** ‚Äì Local vector store
- **GoogleGenerativeAIEmbeddings** ‚Äì For creating embeddings

---

> Built with ‚ù§Ô∏è using Streamlit | Powered by Gemini & LangChain
